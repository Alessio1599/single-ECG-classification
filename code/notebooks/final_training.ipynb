{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_data, class_weights, plot_history\n",
    "from models.CNN import build_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.path.dirname(__file__)\n",
    "base_dir = os.path.dirname(code_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, class_labels, class_names = load_data(base_dir)\n",
    "\n",
    "# Get the class weights\n",
    "class_weights_dict = class_weights(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to run for the last time the hyperparameter tuning, and then I have to choose the final best hyperparameters!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    'layer_1_size': 64,\n",
    "    'layer_2_size': 64,\n",
    "    'layer_3_size': 64,\n",
    "    'layer_FC_size': 64,\n",
    "    'dropout_rate': 0.4,\n",
    "    'learning_rate': 4e-4,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 15 # I've setted to 1 just to compare the result with the other models #20 # I've setted to 5 just to see the performance of the model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train.shape[1], 1) # (187, 1)\n",
    "output_shape = len(np.unique(y_train))\n",
    "\n",
    "model = build_CNN(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    layer_1_size=best_hyperparameters['layer_1_size'],\n",
    "    layer_2_size=best_hyperparameters['layer_2_size'],\n",
    "    layer_3_size=best_hyperparameters['layer_3_size'],\n",
    "    layer_FC_size=best_hyperparameters['layer_FC_size'],\n",
    "    dropout_rate=best_hyperparameters['dropout_rate']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_hyperparameters['learning_rate'])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy', # When the labels are integers\n",
    "    metrics= ['accuracy']\n",
    ")\n",
    "\n",
    "# Define the path to save the model\n",
    "model_save_path = os.path.join(code_dir, 'models/cnn/best_cnn_exam.keras')\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "patience = 5\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=patience,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=best_hyperparameters['batch_size'],\n",
    "    epochs=best_hyperparameters['epochs'],\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "plot_history(history, metric='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
